{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":234911,"sourceType":"datasetVersion","datasetId":99505}],"dockerImageVersionId":30407,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Importing required libraries","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom keras import Sequential\nfrom keras.layers import Dense,Conv2D,MaxPooling2D,Flatten, BatchNormalization,Dropout\nimport matplotlib.pyplot as plt\nfrom keras import callbacks\nimport numpy as np\nfrom keras.applications.vgg16 import VGG16\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score,ConfusionMatrixDisplay\nimport seaborn as sns\nimport cv2\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras.preprocessing.image import img_to_array\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom keras.preprocessing.image import ImageDataGenerator","metadata":{"execution":{"iopub.status.busy":"2024-06-23T21:09:07.602986Z","iopub.execute_input":"2024-06-23T21:09:07.603471Z","iopub.status.idle":"2024-06-23T21:09:07.612831Z","shell.execute_reply.started":"2024-06-23T21:09:07.603404Z","shell.execute_reply":"2024-06-23T21:09:07.611498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Preprocessing","metadata":{}},{"cell_type":"code","source":"batch_size=32\nimg_size=48\ntrain_datagen = ImageDataGenerator(\n                    rescale=1./255, \n                    rotation_range=30,\n                    shear_range=0.3,\n                    #zoom_range=0.3,\n                    width_shift_range = 0.1,\n                    height_shift_range = 0.1,\n                    horizontal_flip=True,\n                    validation_split=0.3)\n\nvalidation_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.3)\n\ntest_datagen = ImageDataGenerator(rescale=1./255)\n\ntrain_generator = train_datagen.flow_from_directory(\n        '/kaggle/input/face-expression-recognition-dataset/images/train/',\n        target_size=(img_size, img_size),\n        color_mode =  'grayscale',\n        batch_size=batch_size,\n        class_mode='categorical',\n        subset = 'training') \n\nvalidation_generator = validation_datagen.flow_from_directory(\n        '/kaggle/input/face-expression-recognition-dataset/images/train/',\n        target_size=(img_size, img_size),\n        color_mode =  'grayscale',\n        batch_size=batch_size,\n        class_mode='categorical',\n        subset='validation')\n\ntest_generator = test_datagen.flow_from_directory(\n        '/kaggle/input/face-expression-recognition-dataset/images/validation/',\n        target_size=(img_size, img_size),\n        color_mode =  'grayscale',\n        batch_size=batch_size,\n        class_mode='categorical',\n        shuffle = False)","metadata":{"execution":{"iopub.status.busy":"2024-06-23T21:09:07.615296Z","iopub.execute_input":"2024-06-23T21:09:07.615795Z","iopub.status.idle":"2024-06-23T21:09:13.569268Z","shell.execute_reply.started":"2024-06-23T21:09:07.615754Z","shell.execute_reply":"2024-06-23T21:09:13.567769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_generator.class_indices","metadata":{"execution":{"iopub.status.busy":"2024-06-23T21:09:13.571030Z","iopub.execute_input":"2024-06-23T21:09:13.571462Z","iopub.status.idle":"2024-06-23T21:09:13.579685Z","shell.execute_reply.started":"2024-06-23T21:09:13.571403Z","shell.execute_reply":"2024-06-23T21:09:13.578347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Tuned Model","metadata":{}},{"cell_type":"code","source":"from keras.optimizers import Adam,SGD,RMSprop\n\ninitializer = tf.keras.initializers.HeUniform(seed=42)\nreg = tf.keras.regularizers.L2(l2=0.01)\nno_of_classes = 7\n\nmodel_tuned = Sequential()\n\n#1st CNN layer\nmodel_tuned.add(Conv2D(64,(3,3),padding = 'same', activation='relu',input_shape = (48,48,1), kernel_initializer=initializer, kernel_regularizer=reg))\nmodel_tuned.add(BatchNormalization())\nmodel_tuned.add(MaxPooling2D(pool_size = (2,2)))\nmodel_tuned.add(Dropout(0.25))\n\n#2nd CNN layer\nmodel_tuned.add(Conv2D(128,(5,5),padding = 'same', activation='relu', kernel_initializer=initializer, kernel_regularizer=reg))\nmodel_tuned.add(BatchNormalization())\nmodel_tuned.add(MaxPooling2D(pool_size = (2,2)))\nmodel_tuned.add(Dropout (0.25))\n\n#3rd CNN layer\nmodel_tuned.add(Conv2D(512,(3,3),padding = 'same', activation='relu', kernel_initializer=initializer, kernel_regularizer=reg))\nmodel_tuned.add(BatchNormalization())\nmodel_tuned.add(MaxPooling2D(pool_size = (2,2)))\nmodel_tuned.add(Dropout (0.25))\n\n#4th CNN layer\nmodel_tuned.add(Conv2D(512,(3,3), padding='same', activation='relu', kernel_initializer=initializer, kernel_regularizer=reg))\nmodel_tuned.add(BatchNormalization())\nmodel_tuned.add(MaxPooling2D(pool_size=(2, 2)))\nmodel_tuned.add(Dropout(0.25))\n\nmodel_tuned.add(Flatten())\n\n#Fully connected 1st layer\nmodel_tuned.add(Dense(256, activation='relu'))\nmodel_tuned.add(BatchNormalization())\nmodel_tuned.add(Dropout(0.25))\n\n\n# Fully connected layer 2nd layer\nmodel_tuned.add(Dense(512, activation='relu'))\nmodel_tuned.add(BatchNormalization())\nmodel_tuned.add(Dropout(0.25))\n\nmodel_tuned.add(Dense(no_of_classes, activation='softmax'))\n\nmodel_tuned.summary()","metadata":{"execution":{"iopub.status.busy":"2024-06-23T21:09:13.581483Z","iopub.execute_input":"2024-06-23T21:09:13.582355Z","iopub.status.idle":"2024-06-23T21:09:13.947807Z","shell.execute_reply.started":"2024-06-23T21:09:13.582305Z","shell.execute_reply":"2024-06-23T21:09:13.946480Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Applying callbacks and compiling","metadata":{}},{"cell_type":"code","source":"from keras.optimizers import RMSprop,SGD,Adam\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n\ncheckpoint_tuned = ModelCheckpoint(\"./model.h5\", monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n\nearly_stopping = EarlyStopping(monitor='val_loss',\n                          min_delta=0,\n                          patience=3,\n                          verbose=1,\n                          restore_best_weights=True\n                          )\n\nreduce_learningrate = ReduceLROnPlateau(monitor='val_loss',\n                              factor=0.2,\n                              patience=3,\n                              verbose=1,\n                              min_delta=0.0001)\n\ncallbacks_list_tuned = [checkpoint_tuned,reduce_learningrate]\n\nepochs = 20\n\nmodel_tuned.compile(loss='categorical_crossentropy',\n              optimizer = Adam(learning_rate=0.0001),\n              metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2024-06-23T21:09:13.952051Z","iopub.execute_input":"2024-06-23T21:09:13.952785Z","iopub.status.idle":"2024-06-23T21:09:13.976706Z","shell.execute_reply.started":"2024-06-23T21:09:13.952729Z","shell.execute_reply":"2024-06-23T21:09:13.975235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Fitting the Model","metadata":{}},{"cell_type":"code","source":"history_tuned = model_tuned.fit(train_generator,\n                                steps_per_epoch=train_generator.n//train_generator.batch_size,\n                                epochs=epochs,\n                                validation_data = validation_generator,\n                                validation_steps = validation_generator.n//validation_generator.batch_size,\n                                callbacks=callbacks_list_tuned\n                                )","metadata":{"execution":{"iopub.status.busy":"2024-06-23T21:09:13.978055Z","iopub.execute_input":"2024-06-23T21:09:13.978547Z","iopub.status.idle":"2024-06-23T23:53:42.326071Z","shell.execute_reply.started":"2024-06-23T21:09:13.978493Z","shell.execute_reply":"2024-06-23T23:53:42.324717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Comparing Loss","metadata":{}},{"cell_type":"code","source":"ã…Œ","metadata":{"execution":{"iopub.status.busy":"2024-06-23T23:53:42.328247Z","iopub.execute_input":"2024-06-23T23:53:42.328642Z","iopub.status.idle":"2024-06-23T23:53:42.558828Z","shell.execute_reply.started":"2024-06-23T23:53:42.328597Z","shell.execute_reply":"2024-06-23T23:53:42.557508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Comparing Accuracy","metadata":{}},{"cell_type":"code","source":"# Comparing accuracy\nplt.plot(history_tuned.history['accuracy'], label='Training accuracy', color='red')\nplt.plot(history_tuned.history['val_accuracy'], label='Validation accuracy', color='blue')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-23T23:53:42.560557Z","iopub.execute_input":"2024-06-23T23:53:42.561022Z","iopub.status.idle":"2024-06-23T23:53:42.802326Z","shell.execute_reply.started":"2024-06-23T23:53:42.560970Z","shell.execute_reply":"2024-06-23T23:53:42.800774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Running Model on Webcam","metadata":{}},{"cell_type":"code","source":"import cv2\nfrom tensorflow.keras.preprocessing.image import img_to_array","metadata":{"execution":{"iopub.status.busy":"2024-06-23T23:53:42.803924Z","iopub.execute_input":"2024-06-23T23:53:42.804550Z","iopub.status.idle":"2024-06-23T23:53:42.811223Z","shell.execute_reply.started":"2024-06-23T23:53:42.804499Z","shell.execute_reply":"2024-06-23T23:53:42.809726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"haar = cv2.CascadeClassifier(\"haarcascade_frontalface_default.xml\")","metadata":{"execution":{"iopub.status.busy":"2024-06-23T23:53:42.812796Z","iopub.execute_input":"2024-06-23T23:53:42.813851Z","iopub.status.idle":"2024-06-23T23:53:42.829756Z","shell.execute_reply.started":"2024-06-23T23:53:42.813808Z","shell.execute_reply":"2024-06-23T23:53:42.828245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def detect_face(img):\n    coord = haar.detectMultiScale(img)\n    \n    return coord","metadata":{"execution":{"iopub.status.busy":"2024-06-23T23:53:42.831556Z","iopub.execute_input":"2024-06-23T23:53:42.831944Z","iopub.status.idle":"2024-06-23T23:53:42.837792Z","shell.execute_reply.started":"2024-06-23T23:53:42.831907Z","shell.execute_reply":"2024-06-23T23:53:42.836525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Use Below code run the model on Webcam and detect Emotions**","metadata":{}},{"cell_type":"code","source":"# # open webcam\n# webcam = cv2.VideoCapture(0)\n    \n# classes = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n\n# # loop through frames\n# while webcam.isOpened():\n\n#     # read frame from webcam \n#     status, frame = webcam.read()\n    \n#     # Converting to grayscale image\n#     gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n\n#     # apply face detection\n#     cood = detect_face(gray_frame)\n\n#     # loop through detected faces\n#     for x,y,w,h in cood:\n#         cv2.rectangle(frame,(x,y),(x+w,y+h), (0,255,0),3)\n\n#         # crop the detected face region\n#         face_crop = np.copy(gray_frame[y:y+h,x:x+w])\n\n#         if (face_crop.shape[0]) < 10 or (face_crop.shape[1]) < 10:\n#             continue\n\n#         # preprocessing for emotion detection model\n#         face_crop = cv2.resize(face_crop, (48,48))\n#         face_crop = face_crop.astype(\"float\") / 255.0\n#         face_crop = img_to_array(face_crop)\n#         face_crop = np.expand_dims(face_crop, axis=0)\n\n#         # apply emotion detection on face\n#         conf = model_tuned.predict(face_crop)[0] # model.predict return a 2D matrix, ex: [[9.9993384e-01 7.4850512e-05]]\n\n#         # get label with max accuracy\n#         idx = np.argmax(conf)\n#         label = classes[idx]\n\n#         #label = \"{}: {:.2f}%\".format(label, conf[idx] * 100)\n\n#         #Y = x - 10 if y - 10 > 10 else y + 10\n\n#         # write label and confidence above face rectangle\n#         cv2.putText(frame, label, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX,\n#                     0.7, (0, 255, 255), 2)\n\n#     # display output\n#     cv2.imshow(\"Emotion Detection\", frame)\n\n#     # press \"Q\" to stop\n#     if cv2.waitKey(1) & 0xFF == ord('q'):\n#         break\n\n# # release resources\n# webcam.release()\n# cv2.destroyAllWindows()","metadata":{"execution":{"iopub.status.busy":"2024-06-23T23:53:42.839675Z","iopub.execute_input":"2024-06-23T23:53:42.840165Z","iopub.status.idle":"2024-06-23T23:53:42.854176Z","shell.execute_reply.started":"2024-06-23T23:53:42.840116Z","shell.execute_reply":"2024-06-23T23:53:42.852834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***Please upvote if you like my work***","metadata":{}}]}